ai_router.py:
"""
"""
AI Router Module

This module handles all AI-related endpoints, including message processing
and graph manipulation through natural language commands.
"""
from fastapi import APIRouter, HTTPException, status
from app.schemas.schemas import MessageRequest, MessageResponse
import logging
import subprocess
import json
import os

# Configure logging
logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/ai",
    tags=["AI"],
    responses={
        404: {"description": "Not found"},
        500: {"description": "Internal server error"},
    }
)

@router.post(
    "/message",
    response_model=MessageResponse,
    summary="Process a user message",
    description="""
    Process a message from the user and return an AI response along with any graph updates.
    
    The endpoint accepts natural language commands such as:
    - "add node": Creates a new node
    - "remove node X": Removes node number X
    - "list nodes": Lists all current nodes
    
    The response includes:
    - AI's text response
    - Any new or updated nodes
    - Any new or updated edges between nodes
    """,
    responses={
        200: {
            "description": "Successful response",
            "content": {
                "application/json": {
                    "example": {
                        "ai_response": "Node 1 added successfully.",
                        "nodes": [{
                            "id": "123e4567-e89b-12d3-a456-426614174000",
                            "type": "default",
                            "data": {"label": "Node 1"},
                            "position": {"x": 250.0, "y": 150.0}
                        }],
                        "edges": []
                    }
                }
            }
        },
        400: {
            "description": "Bad request",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "User message cannot be empty."
                    }
                }
            }
        }
    }
)
async def process_message(request: MessageRequest):
    logger.info(f"Processing user message: {request.user_message}")
    try:
        # Validate user message
        if not request.user_message.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="User message cannot be empty."
            )

        # Paths to scripts and session directory
        main_script_path = '/Users/ysharm12/Documents/orchestra/AI/main.py'
        session_dir = '/Users/ysharm12/Documents/orchestra/AI/sessions/static_session'

        # Ensure the session directory exists
        os.makedirs(session_dir, exist_ok=True)

        # Run main.py as a subprocess with the user message
        result = subprocess.run(
            ['python3', main_script_path, request.user_message],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            # Handle error
            logger.error(f"Error running main.py: {result.stderr}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Error processing the AI response."
            )

        # Read the api_response.json file generated by main.py
        api_response_file = os.path.join(session_dir, 'api_response.json')
        if os.path.exists(api_response_file):
            with open(api_response_file, 'r') as f:
                api_response = json.load(f)
        else:
            logger.error("api_response.json not found.")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="AI response not found."
            )

        # Construct the MessageResponse object
        response = MessageResponse(
            ai_response=api_response.get('ai_response', ''),
            nodes=api_response.get('nodes', []),
            edges=api_response.get('edges', [])
        )

        logger.info(f"Successfully generated AI response for user message: {request.user_message}")
        return response

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        # Log the actual error but return a generic message
        logger.error(f"Error processing AI message: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An error occurred while processing your request. Please try again later."
        )
"""


main.py:
"""
# /Users/ysharm12/Documents/orchestra/AI/main.py

import asyncio
import os
import sys
import uuid
import re
import subprocess
from parent_agent import process_user_input
from llm import call_parent

async def main():
    # Check if user input is provided as a command-line argument
    if len(sys.argv) > 1:
        user_input = sys.argv[1]
    else:
        # Interactive mode
        user_input = input("\nEnter your query or modification request: ")

    # Use a static session ID (can be made dynamic if needed)
    session_id = 'static_session'

    # Create a session directory
    session_dir = os.path.join('/Users/ysharm12/Documents/orchestra/AI/sessions', session_id)
    os.makedirs(session_dir, exist_ok=True)

    ai_response, scripts = await process_user_input(user_input, session_dir)

    if scripts:
        # Save scripts to files
        await save_scripts(scripts, session_dir)

    # Save the AI response
    await save_ai_response(ai_response, session_dir)

    # Update the graph
    await run_graph_builder(session_dir)

    # Note: Do not print any additional output that could interfere with ai_router.py
    
async def save_scripts(scripts, session_dir):
    """
    Saves the scripts to files in the session directory.
    Removes older versions when a script is updated.
    """
    for script in scripts:
        filename = script["filename"]
        version = script["version"]
        content = script["content"]

        # Remove older versions of the script
        existing_files = [f for f in os.listdir(session_dir) if f == filename]
        for existing_file in existing_files:
            os.remove(os.path.join(session_dir, existing_file))

        # Save the new script
        script_path = os.path.join(session_dir, filename)
        with open(script_path, 'w') as f:
            # Add the header with filename and version
            header = f"# {filename} (Version {version})\n"
            f.write(header + '\n' + content)

        print(f"Saved script: {filename} (Version {version})")

async def save_ai_response(ai_response, session_dir):
    """
    Saves the AI's response to the user in the session directory.
    """
    ai_response_file = os.path.join(session_dir, 'ai_response.txt')
    with open(ai_response_file, 'w') as f:
        f.write(ai_response)

async def run_graph_builder(session_dir):
    """
    Runs the graph_builder.py script to generate the API response.
    """
    # Path to graph_builder.py
    graph_builder_path = os.path.join('/Users/ysharm12/Documents/orchestra/AI', 'graph_builder.py')

    try:
        result = subprocess.run(
            ['python3', graph_builder_path, session_dir],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            print("Graph builder failed:")
            print(result.stderr)
    except Exception as e:
        print(f"An error occurred while running graph_builder.py: {e}")

if __name__ == "__main__":
    asyncio.run(main())
"""


graph_builder.py:
"""
# graph_builder.py

import os
import re
import json
import asyncio
import sys
import shutil
from llm import call_graph

async def read_scripts(session_dir):
    """
    Reads all the scripts in the session directory and returns them as a list.
    """
    scripts = []
    script_files = sorted([
        f for f in os.listdir(session_dir) if f.endswith('.py')
    ])

    for filename in script_files:
        filepath = os.path.join(session_dir, filename)
        with open(filepath, 'r') as f:
            code = f.read()
        version = await extract_version(code) or 1
        scripts.append({
            'filename': filename,
            'version': version,
            'content': code
        })
    return scripts

async def extract_version(script_content):
    """
    Extracts the version number from the script content.
    """
    version_pattern = r'#.*\(Version\s*(\d+)\)'
    match = re.search(version_pattern, script_content)
    if match:
        return int(match.group(1))
    else:
        return None
    
async def generate_graph(scripts, ai_response):
    """
    Generates the graph JSON including the AI response.
    """
    # Prepare the prompt for the LLM
    prompt = f"""
You are an assistant that generates a JSON response for a UI to display a graph.
The graph represents scripts and their relationships.

Here are the scripts:

"""

    for script in scripts:
        prompt += f"\n# {script['filename']} (Version {script['version']})\n```python\n{script['content']}\n```\n"

    prompt += f"""
The AI has the following response to the user:

"{ai_response}"

Your task is to analyze the scripts and generate a JSON response that represents the scripts and their relationships.

The API response must follow this structure:

{{
    "ai_response": "Text response to user",
    "nodes": [...],
    "edges": [...],
}}

Ensure that:
- The "ai_response" field contains the AI's response to the user.
- Nodes and edges accurately represent the relationships between scripts.
- Positions are assigned to nodes to make the graph visually appealing.

Each node must have these properties:
- "id": Unique string ID
- "type": Must be "custom"
- "position": An object with "x" and "y" coordinates (floats)
- "data": An object containing:
    - "label": Display name (script filename)
    - "type": One of: "input", "process", "output"
    - "description": Optional description
    - "inputs": Array of input handles, MUST use "input-0" format
    - "outputs": Array of output handles, MUST use "output-0" format
    - "color": Optional color

Each edge must have these properties:
- "id": Unique string ID (suggested format: "e{{source}}-{{target}}")
- "source": Source node ID (string)
- "target": Target node ID (string)
- "sourceHandle": Must match source node's outputs array
- "targetHandle": Must match target node's inputs array
- "animated": Optional, defaults to true

Important Rules:

1. Handle IDs:
   - Always use zero-based indices: "input-0", "output-0"
   - Multiple handles should increment: "input-0", "input-1", etc.
   - Handles in edges must match the node's inputs/outputs arrays

2. Node Types:
   - "input" nodes should only have outputs
   - "output" nodes should only have inputs
   - "process" nodes can have both inputs and outputs

Return only the JSON response without any extra text or comments. The JSON must be valid and parseable.
"""

    # Call the LLM to generate the graph
    llm_response = await call_graph(prompt)
    json_response = await extract_json(llm_response)

    if json_response:
        return json_response
    else:
        print("Failed to generate JSON response from LLM.")
        return None

async def generate_additional_json(scripts, graph_json):
    """
    Generates return_json.json and node_descriptions.json based on graph_json and scripts.
    """
    # Generate return_json.json (which might be similar to api_response)
    # For this example, we'll assume return_json.json is similar to api_response

    return_json = graph_json  # Or process it differently if needed

    # Save return_json.json
    return_json_file = os.path.join(session_dir, 'return_json.json')
    with open(return_json_file, 'w') as f:
        json.dump(return_json, f, indent=4)

    # Generate node_descriptions.json
    # Prepare the prompt for the LLM
    prompt = f"""
You are an assistant that generates HTML descriptions for nodes in a graph, using Tailwind CSS classes.

Here is the graph JSON:

{json.dumps(graph_json, indent=4)}

Here are the scripts:

"""
    for script in scripts:
        prompt += f"\n# {script['filename']} (Version {script['version']})\n```python\n{script['content']}\n```\n"

    prompt += """
Your task is to create a JSON object called node_descriptions, where each key is the node ID, and each value is an object containing:

{
    "description_html": "<HTML content using Tailwind CSS classes>"
}

Instructions:

- For each node in the graph JSON, generate an HTML description that provides an overview of the node.
- Use the script content to inform the description.
- The HTML should be well-structured and styled using Tailwind CSS classes.
- Ensure that the node IDs in node_descriptions match the IDs in the graph JSON.
- Return only the JSON object without any extra text or comments.

Example format:

{
    "node_id_1": {
        "description_html": "<div class='...'>...</div>"
    },
    "node_id_2": {
        "description_html": "<div class='...'>...</div>"
    },
    ...
}
"""

    # Call the LLM to generate the node descriptions
    llm_response = await call_graph(prompt)
    node_descriptions = await extract_json(llm_response)

    if node_descriptions:
        # Save node_descriptions.json
        node_descriptions_file = os.path.join(session_dir, 'node_descriptions.json')
        with open(node_descriptions_file, 'w') as f:
            json.dump(node_descriptions, f, indent=4)
    else:
        print("Failed to generate node descriptions from LLM.")

async def extract_json(text):
    """
    Extracts JSON content from the LLM response.
    """
    # Try to find the first occurrence of '{' and the last occurrence of '}' and extract the JSON
    try:
        json_start = text.find('{')
        json_end = text.rfind('}') + 1
        json_text = text[json_start:json_end]
        json_response = json.loads(json_text)
        return json_response
    except (ValueError, json.JSONDecodeError) as e:
        print(f"JSON decode error: {e}")
        return None

async def main():
    if len(sys.argv) != 2:
        print("Usage: python3 graph_builder.py <session_dir>")
        return

    session_dir = sys.argv[1]

    # Read the AI response from the session folder
    ai_response_file = os.path.join(session_dir, 'ai_response.txt')
    if os.path.exists(ai_response_file):
        with open(ai_response_file, 'r') as f:
            ai_response = f.read().strip()
    else:
        ai_response = ""

    scripts = await read_scripts(session_dir)
    api_response = await generate_graph(scripts, ai_response)

    if api_response:
        # Save api_response.json as before
        output_file = os.path.join(session_dir, 'api_response.json')
        with open(output_file, 'w') as f:
            json.dump(api_response, f, indent=4)

        # Also generate additional JSON files
        await generate_additional_json(scripts, api_response)

        # Copy the new JSON files to the specified directory
        target_dir = '/Users/ysharm12/Documents/orchestra/orchestra/orchestra-backend/app/data/'
        os.makedirs(target_dir, exist_ok=True)

        # Copy return_json.json
        return_json_file = os.path.join(session_dir, 'return_json.json')
        shutil.copy(return_json_file, target_dir)

        # Copy node_descriptions.json
        node_descriptions_file = os.path.join(session_dir, 'node_descriptions.json')
        shutil.copy(node_descriptions_file, target_dir)

    else:
        print("No valid API response generated.")

if __name__ == "__main__":
    asyncio.run(main())
"""
